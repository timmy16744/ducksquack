---
title: "The Widening Gyre"
date: "2026-01-17"
color: "purple"
---

I want to begin with something I believe deeply: that these tools should be accessible to everyone. That anyone willing to engage seriously with artificial intelligence ought to be able to learn it, to benefit from it, to integrate it into their work and thinking. This conviction matters to me. It is, in some sense, the reason I find the present moment so troubling.

Because I must confess that what I'm observing has started to challenge my optimism.

I've been tracking adoption patterns rather carefully, and the disparity between insiders and outsiders has become extraordinary. Perhaps unprecedented. In certain circles, people have genuinely restructured their lives around orchestrated networks of AI agents. They consult these systems before nearly every decision. The degree of cognitive integration they've achieved was, until quite recently, the province of speculative fiction. And not the utopian variety.

Then there is everywhere else. Knowledge workers petitioning for approval to use a basic assistant in their workplace software. Entire organisations where AI remains an abstraction, something other people are doing, somewhere else, in a future that may or may not arrive. I should note that the institutions imposing these restrictions are not acting irrationally. They face genuine concerns: security, compliance, liability, data governance. The locally rational choice is often caution. The tragedy is structural. Individual organisations making defensible decisions may collectively produce an outcome that damages the very people those decisions were meant to protect.

One could argue that early adopter communities have always felt this intense from within. That's possible. I'm less certain than I once was. What I suspect is that we're witnessing a cultural phase transition running in parallel with the technical one. These are distinct phenomena, and both demand attention.

Let me be precise about what's being acquired inside this bubble, because I think there's a tendency to trivialise it. When people dismiss AI fluency as "just learning to use ChatGPT," they fundamentally misunderstand the nature of the skill.

What early adopters are developing is tacit knowledge. It resists explicit articulation. It's the intuition for what a model can and cannot do. The craft of constructing prompts that elicit genuine insight rather than plausible noise. The judgment to know when to trust an output and when to probe further. The habit, now deeply ingrained, of reaching for AI as a first resort rather than a last one.

This is not a discrete skill one acquires in an afternoon tutorial. It is closer to a disposition, a way of thinking, that compounds over months of daily practice. Those inside the bubble have been accumulating this tacit knowledge for years now. They have built mental models, developed workflows, made thousands of small errors and corrections that have refined their intuitions. The knowledge is, by its nature, difficult to transfer. One can explain prompting techniques, certainly. One can share workflows. But the underlying intuition? That requires immersion. There is no shortcut I'm aware of.

Here is what concerns me most about the present situation. The gap is not stable.

This seems obvious once stated, but its implications are severe. People using AI fluently become better at using AI. Their skills compound. They build workflows upon workflows, integrate tools into tools, develop intuitions that enable them to extract more value from each new capability that emerges. Each month of fluency creates new distance from those still waiting for approval.

At this point, one might reach for historical comfort. The printing press. The internet. The spreadsheet. Previous technological divides eventually narrowed. Literacy became universal. Digital natives emerged. Surely AI will follow the same trajectory? I take this argument seriously. It deserves more than dismissal. And yet I'm uncertain whether the comparison holds. Previous technologies eventually stabilised. The gap between expert and novice users of a spreadsheet, while real, became manageable. One could catch up. The target held still long enough to be reached. AI is not holding still. The capabilities are improving faster than the pedagogical infrastructure can adapt. By the time we've developed good curricula for today's tools, tomorrow's tools have arrived with entirely new affordances. The early adopters ride this wave. Everyone else watches it recede.

We've seen how path dependence operates in this domain. The organisations that understood scaling laws early and accumulated compute before 2022 established advantages that proved, in practice, insurmountable. The gap between frontier labs and everyone else did not narrow as the technology matured. It widened. I find myself wondering whether something similar is happening at the individual level. Whether we have crossed some threshold beyond which catching up becomes, if not impossible, then so costly as to be practically equivalent.

Perhaps there's reason for hope. Perhaps interfaces will become so intuitive that current fluency becomes irrelevant, a quaint artifact of the early days when one needed special knowledge to extract value. I want to believe this. I'm not yet persuaded by it.

I've been speaking about access and institutions, but I would be dishonest if I didn't acknowledge another dimension of this divide. One that's more uncomfortable to discuss.

There is fear here. Genuine fear.

Knowledge workers whose expertise may be disrupted have emotional reasons to disengage from AI. To minimise its significance. To find reasons why it won't affect their domain, their work, their identity. This is not stupidity. It is not laziness. It is a deeply human response to perceived threat. There is also overwhelm. The pace of change is genuinely disorienting. New models arrive monthly. Capabilities expand in unpredictable directions. For someone already stretched thin by the demands of their work, the prospect of adding "learn to use AI" to their responsibilities can feel impossible. Easier, perhaps, to wait. To hope the dust settles.

I have compassion for this. I think we must have compassion for this, those of us inside the bubble. The alternative is a kind of contempt that helps no one and understands nothing.

But compassion alone doesn't resolve the underlying problem. And this brings me to a question I don't know how to answer: what do we owe?

Those of us who have had the access and the inclination and the time to develop fluency. Is there a duty to bridge this gap? To teach, to share, to advocate for broader access? Or is that paternalistic, a kind of technological noblesse oblige that assumes others need our help? I genuinely feel the pull of both positions. What I do know is that the present trajectory troubles me. A world cleaving into those who can leverage these tools and those who cannot. A gap that compounds daily. A future arriving unevenly.

I began by saying I believe these tools should be accessible to everyone. I absolutely believe that. But I'm not certain that belief, however sincere, will be enough to make it true.
